# 技術設計書

## 1. 概要

本ドキュメントは、アプリケーション自動生成プラットフォームの実装にあたり、技術選定とその具体的な設計方針を定義するものです。保守性、拡張性、および開発効率を最大化することを目的とし、以下の主要な設計原則に基づいています。

- **抽象化**: 特定の技術やプロバイダーへの依存を最小限に抑える。
- **型安全性**: データフロー全体で型を保証し、実行時エラーを削減する。
- **設定の外部化**: 環境ごとの差異や機能の切り替えをコードから分離する。
- **コンテナ化**: 開発環境と本番環境の差異をなくし、デプロイを簡素化する。

## 2. LLM統合アーキテクチャ

### 2.1 LLMプロバイダーの抽象化

**目的:** OpenAI, Azure OpenAIなど、複数のLLMプロバイダーを容易に切り替えられるようにし、将来的な拡張性も確保します。

**設計方針:**
- **LangChainの活用:** `langchain-openai` パッケージを利用し、`ChatOpenAI` クラスを介してOpenAIとAzure OpenAIを統一的に扱います。
- **Factoryパターン:** LLMインスタンスを生成するFactory関数を導入し、呼び出し側は具体的なプロバイダーを意識しない設計とします。
- **設定ファイルによる切り替え:** `config/llm.yaml` でアクティブなプロバイダーを指定します。

**実装例: LLM Factory**

```python
# backend/app/llm/factory.py

from langchain_openai import ChatOpenAI
from pydantic import BaseModel

class LLMConfig(BaseModel):
    provider: str
    model_name: str
    # ...その他の設定

def create_llm(config: LLMConfig) -> ChatOpenAI:
    if config.provider == "openai":
        return ChatOpenAI(model=config.model_name, ...)
    elif config.provider == "azure_openai":
        return ChatOpenAI(azure_deployment=..., ...)
    else:
        raise ValueError(f"Unsupported provider: {config.provider}")
```

## 3. 型安全なデータハンドリング (Structured Output)

**目的:** LLMからの応答を構造化データとして型安全に受け取り、アプリケーション全体でのデータの整合性を保証します。

**設計方針:**
- **Pydanticモデル:** LLMに期待する出力のスキーマをPydanticモデルとして厳密に定義します。
- **LangChain `with_structured_output`:** LangChainのこの機能を利用し、LLMのFunction CallingやJSON Modeを介して、Pydanticモデルに準拠した出力を強制します。

**実装例: アプリケーション仕様書のスキーマ定義**

```python
# backend/app/schemas/application_spec.py

from pydantic import BaseModel, Field
from typing import List, Literal

class Requirement(BaseModel):
    id: str = Field(description="要件ID（例: R1, R2）")
    description: str = Field(description="要件の説明")
    type: Literal["input", "processing", "output"]

class ApplicationSpec(BaseModel):
    """LLMによって生成されるアプリケーション仕様書のスキーマ"""
    app_name: str = Field(description="アプリケーション名")
    app_type: Literal["TYPE_CRUD", "TYPE_VALIDATION"]
    requirements: List[Requirement]

# エージェントでの利用
structured_llm = llm.with_structured_output(ApplicationSpec)
spec: ApplicationSpec = structured_llm.invoke(prompt)
```

## 4. 設定の一元管理

**目的:** モック/実装の切り替え、機能フラグ、外部サービス（Dify等）の定義など、環境に依存する設定をコードから分離し、一元管理します。

**設計方針:**
- **YAMLベースの設定ファイル:** `config/` ディレクトリ内に、機能ごとに分割したYAMLファイルで設定を管理します。
- **ConfigManagerクラス:** 設定ファイルを読み込み、アプリケーション全体に提供するシングルトンクラスを実装します。

**設定ファイルの構造:**

```
config/
├── features.yaml         # 機能の有効/無効、モック切り替え
├── llm.yaml              # LLMプロバイダー設定
└── dify.yaml             # Difyワークフロー、ナレッジベース定義
```

**実装例: `features.yaml`**

```yaml
# 現在の実行フェーズ（この値に応じて各機能の挙動が変わる）
phase: "mvp"  # "mvp", "llm_integration", "dify_integration"

# エージェント層のモック設定
agents:
  use_mock: true  # Phase 1ではtrue、Phase 2以降はfalse

# Dify統合のモック設定
dify:
  use_mock: true  # Phase 3でもテスト時はtrueにできる
```

**実装例: `dify.yaml`**

```yaml
# Difyワークフローカタログ
workflows:
  - id: "upload_and_process"
    name: "ファイルアップロードと処理"
    dify_workflow_id: "wf_abc123" # 実際のDify上のID
    description: "PDFをアップロードして構造化データを抽出"

  - id: "create_item"
    name: "アイテム作成"
    dify_workflow_id: "wf_ghi789"
    description: "新規アイテムをナレッジベースに登録"
```

## 5. 環境とデプロイ

**目的:** Docker Composeを用いて、開発環境と本番環境の差異をなくし、誰でも容易に環境を構築・起動できるようにします。

**設計方針:**
- **マルチコンテナ構成:** `frontend`（React）、`backend`（FastAPI）のサービスを定義した `docker-compose.yml` をプロジェクトルートに配置します。
- **ボリュームマウントによるホットリロード:** 開発時にはソースコードをコンテナにマウントし、コード変更が即座に反映されるようにします。
- **`.env`ファイルによる機密情報管理:** APIキーなどの機密情報を `.env` ファイルに記述し、Docker Compose経由で環境変数としてコンテナに渡します。

**実装例: `docker-compose.yml`**

```yaml
version: '3.8'

services:
  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app

  backend:
    build: ./backend
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
      - ./config:/app/config      # 設定ファイルをマウント
      - ./output:/app/output      # 生成物出力先をマウント
    env_file:
      - .env
```

## 6. 生成アプリケーションの管理

**目的:** 生成されたアプリケーション（デモアプリ）を、管理しやすく、かつ再利用可能な形式で保存・配布します。

**設計方針:**
- **配布形式:** アプリケーションのソースコード一式、Dockerfile、`docker-compose.yml` を含んだ **ZIPファイル**として配布します。
- **専用フォルダでの管理:** `output/` ディレクトリ以下に、ユーザーIDとプロジェクトIDで階層化して生成物を管理します。
- **メタデータの併存:** 生成されたZIPファイルと共に、生成時の要件や設定を記録した `metadata.json` を保存し、トレーサビリティを確保します。

**出力ディレクトリ構造:**

```
output/
└── {user_id}/
    └── {project_id}/
        ├── app.zip         # 生成されたアプリケーション本体
        └── metadata.json   # 生成時のメタデータ
```

**実装例: パッケージ化エンジン**

```python
# backend/app/core/packaging.py
import zipfile
from pathlib import Path

class PackagingEngine:
    def package_app(self, generated_code_dir: Path, output_path: Path):
        with zipfile.ZipFile(output_path, "w", zipfile.ZIP_DEFLATED) as zipf:
            for file_path in generated_code_dir.rglob("*"):
                arcname = file_path.relative_to(generated_code_dir)
                zipf.write(file_path, arcname)
        return output_path
```
